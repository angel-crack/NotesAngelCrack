This topic will show the methods used to capture and transmit video over the network and the differences between different codecs.

## Frames, Pixels and Resolution

![[Pasted image 20241107083611.png]]

Before you look at video standards in more detail, there are a few terms you need to understand. Firstly a "frame" is an image made up of dots called "pixels." The picture size, or resolution, is the number of pixels across the width by the number of pixels vertically.

Resolutions are not restricted to telepresence. Your PC has a screen resolution that it is using now. If you right-click your desktop and go to Properties, then select the Settings tab, you can adjust your screen resolution.

Different video standards use different screen resolutions. While a higher screen resolution provides a higher picture quality, it also needs a lot more bandwidth to send. If you send more bandwidth than the network can handle, then not all the information will arrive at the far end, and actually, the picture quality will degrade significantly.

Quality is also dependent on the size of the screen you are using. Within the figure, the motorcycle looks very clear in the top picture, but if you keep blowing it up and it will look awful. Your system will negotiate the best resolution based on the bandwidth you made the call with. But if you receive a poor quality image, then redialing the call at a lower bandwidth might improve it.

## Video Coding

![[Pasted image 20241107083622.png]]

Video codecs all fundamentally work the same way at the start. They take a single frame of the video, group pixels into blocks, and then group blocks into macroblocks. Where codecs differ is how this grouping process is performed and the quantity of each element used in the groupings.

Macroblocks that are in a contiguous row are grouped into slices (a single row of macroblock).

To reduce the amount of data needed to create a video, video codecs are designed to identify changes from one frame to the next and only update the changes. Initially, the process requires a full-frame, or otherwise known as an I-Frame (Intra-coded), to be used as the starting point, the second frame is compared to the first to identify which macroblocks have changed, and only those blocks are updated and transmitted in the form of P-Frames (predictive-coded) or B-frames (bidirectional predictive-coded).

Pictures, or frames, are grouped into a group of pictures (GOP), with the I-frame as the starting point and P-frames following it to the next I-frame.

![[Pasted image 20241107083629.png]]

The main reason why video applications are more loss-sensitive than voice is that the codecs that are used in video compression work differently from the way that voice codecs work.

Commonly used video codecs, such as MPEG-2, MPEG-4, H.264, and H.265, use temporal compression algorithms. A codec that uses temporal compression does not send a complete frame sample (called an I-frame or keyframe) at every sampling interval. Only some of the frames that are sent are I-frames. Between the full frames (I-frames), only the differences with the previous frame, represented as motion vectors and prediction errors, are encoded. The frames that carry these frame deltas (P- or B-frames) tend to be much smaller than I-frames, which is how the compression algorithm reduces the bandwidth.

With the temporal compression algorithm, a spatial compression algorithm is typically used on each frame to reduce the number of bytes that is necessary to encode the frame itself. This process is similar to the type of encoding that is used in picture compression methods such as JPEG.

![[Pasted image 20241107083636.png]]

What does this mean for the loss tolerance of video traffic?

1. At the commonly used 30-f/s frame rate, a frame is sent every 33 ms. Depending on the resolution and spatial compression that is used, each frame is broken down into several packets, and these packets are transmitted onto the network in a short burst.
    

What happens if you lose a single packet out of this burst?

1. To begin, losing a single packet means losing the complete frame, so the loss is magnified by the fact that a sample is not a single packet as it is with voice. Next, if only spatial compression would be used, then a new frame would arrive after 33 ms, and you would experience a 33-ms freeze. However, due to the temporal encoding scheme, you will not always receive a new I-frame as the next frame. If you lose an I-frame, it can take several hundred milliseconds (depending on the codec) before you get a new I-frame. If you lose a P- or B-frame, the effect is slightly less severe, but this loss will still translate to clearly visible artifacts in subsequent frames. Therefore, from a network design standpoint, to provide a good user experience for video applications, you should design it to be as close to lossless as possible for the video traffic.
    

Another related design objective is to design the network with very high availability in mind. A commonly used target for network reconvergence in the campus is below 200 ms. If a media application loses 200 ms worth of packets, this loss is definitely noticeable to the user, but it will generally be accepted because it is only an incident. A longer period of loss for media applications is noticeable and will detract from the user experience.

## Video Codec Selection

The following are some video codec considerations:

- Transcoding
    
- Bitrate
    
- Quality
    
- Network latency and reliability
    
- Endpoint support
    
- Complexity
    

When choosing a codec, especially for off-network calls, you must consider the following:

- Which codecs are supported on the off-network endpoints? If possible, you should avoid the need for transcoding, especially video transcoding. Transcoding requires dedicated hardware resources. Therefore, try to choose common codecs, if possible. If endpoints do not have a codec in common, then a transcoder is required.
    
- Is there enough bandwidth for the desired number of audio and video calls?
    
- Is quality of service (QoS) implemented? What latency, jitter, and packet loss do I have to expect?
    
- What are the desired audio and video quality?
    

## Video Codecs Compared

||H.264|H.265|
|---|---|---|
|Other names|MPEG-4 AVC (Advanced Video Coding)|HEVC (High-Efficiency Video Coding)|
|Released|2003|Evolved from H.264 – ratified in 2013|
|Bitrate|Half the bitrate of H.263 (MPEG-2)|Half the bitrate of H.264|
|Complexity|Same complexity of H.263 (MPEG-2)|More complex than H.264, which requires more processing|
|Resolution|Supports 8K UHD (8192x4320)|Supports 8K UHD (8192x4320)|

The two leading video codecs are H.264 and H.265. Although H.264 is still generally the de-facto standard, H.265 has some benefits.

**H.264**, or otherwise known as MPEG-4 AVC (Advanced Video Coding), was released in 2003 and is used widely by most video hosting companies, including Netflix, Google, and YouTube. It set itself apart from its predecessor, H.263 (MPEG-2), by reducing the bitrate by half while still maintaining the same quality. Or it is alternatively increasing the quality substantially while maintaining the same storage usage as H.263. It was able to achieve this benefit without increasing the processing requirements or complexity. It supports Spatial and Temporal compression and supports resolutions up to 8K Ultra High-Definition with a maximum resolution of 8192x4320.

**H.265** evolved from H.264 and was ratified in 2013. The bitrate was halved again when compared to its predecessor, but the complexity increased, requiring a lot more processing power to encode and decode H.265. All other values of H.265 remained the same when compared to H.264.